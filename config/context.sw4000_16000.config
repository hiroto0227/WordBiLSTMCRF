### use # to comment out the configure item

### I/O ###
train_dir=/home/sekine/WordLevelChemdner/Repository/SeqData/train.bioes
dev_dir=/home/sekine/WordLevelChemdner/Repository/SeqData/valid.bioes
test_dir=/home/sekine/WordLevelChemdner/Repository/SeqData/test.bioes
model_dir=/home/sekine/WordLevelChemdner/Repository/TrainedModel/contextual_4000_16000
word_emb_dir=/home/sekine/WordLevelChemdner/Repository/Pretrained/watanabe.word2vec.dim50

### SubWord ###
sw_num=2
sentence_piece_dirs=[/home/sekine/WordLevelChemdner/Repository/SentencePieceModel/sp4000.model, /home/sekine/WordLevelChemdner/Repository/SentencePieceModel/sp16000.model]
sw_emb_dirs=[/home/sekine/WordLevelChemdner/Repository/Pretrained/gv_pretrain_sp4000.model ,/home/sekine/WordLevelChemdner/Repository/Pretrained/gv_pretrain_sp16000.model]

### Normalize ###
norm_word_emb=False
norm_char_emb=False
number_normalized=True
seg=True
word_emb_dim=50
char_emb_dim=30
sw_emb_dim=50

###NetworkConfiguration###
use_crf=True
use_char=True
word_seq_feature=LSTM
char_seq_feature=LSTM

###TrainingSetting###
status=train
optimizer=SGD
iteration=50
batch_size=10
ave_batch_loss=False

###Hyperparameters###
char_hidden_dim=50
sw_hidden_dim=50
hidden_dim=200
dropout=0.5
lstm_layer=1
bilstm=True
learning_rate=0.015
lr_decay=0.0001
momentum=0
l2=1e-8
gpu=True
clip=5.0
